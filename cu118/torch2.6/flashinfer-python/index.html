<!DOCTYPE html>
<h1>FlashInfer Python Wheels for CUDA 11.8 + torch 2.6</h1>
<a href="https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2/flashinfer_python-0.2.2+cu118torch2.6-cp38-abi3-linux_x86_64.whl#sha256=9f30407129863aa0f7f877b564bce2beca2ba780574364bd272f69bac10aa8a5">flashinfer_python-0.2.2+cu118torch2.6-cp38-abi3-linux_x86_64.whl</a><br>
<a href="https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2.post1/flashinfer_python-0.2.2.post1+cu118torch2.6-cp38-abi3-linux_x86_64.whl#sha256=ca2c102e369330141e452348cd6ddac4e3f30cf76d865e0cc94d1be7fadb0965">flashinfer_python-0.2.2.post1+cu118torch2.6-cp38-abi3-linux_x86_64.whl</a><br>
