<!DOCTYPE html>
<h1>FlashInfer Python Wheels for CUDA 12.4 + torch 2.6</h1>
<a href="https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2/flashinfer_python-0.2.2+cu124torch2.6-cp38-abi3-linux_x86_64.whl#sha256=5e1cdb2fb7c0e9e9a2a2241becc52b771dc0093dd5f54e10f8bf612e46ef93a9">flashinfer_python-0.2.2+cu124torch2.6-cp38-abi3-linux_x86_64.whl</a><br>
<a href="https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.2.post1/flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl#sha256=7b5950853a0769809199f4f252eb271f63700e4f8a51e0da582f0f066b22cd7c">flashinfer_python-0.2.2.post1+cu124torch2.6-cp38-abi3-linux_x86_64.whl</a><br>
